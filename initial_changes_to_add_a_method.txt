diff --git a/Algorithms/LOOK/Dockerfile b/Algorithms/LOOK/Dockerfile
deleted file mode 100644
index 886a4c17..00000000
--- a/Algorithms/LOOK/Dockerfile
+++ /dev/null
@@ -1,18 +0,0 @@
-
-FROM r-base:4.0.2
-
-LABEL Maintainer="Eric Kernfeld <ekernfe1@jh.edu>"
-
-USER root
-
-WORKDIR /
-
-COPY runLOOK.R /
-COPY rlookc_0.1.0.tar.gz /
-
-RUN R -e "install.packages(c('knockoff', 'data.table'))"
-RUN R -e "install.packages('rlookc_0.1.0.tar.gz', type = 'source')"
-
-RUN mkdir data/
-
-RUN apt-get update && apt-get install time
diff --git a/Algorithms/LOOK/rlookc_0.1.0.tar.gz b/Algorithms/LOOK/rlookc_0.1.0.tar.gz
deleted file mode 100644
index a55d8110..00000000
Binary files a/Algorithms/LOOK/rlookc_0.1.0.tar.gz and /dev/null differ
diff --git a/Algorithms/LOOK/runLOOK.R b/Algorithms/LOOK/runLOOK.R
deleted file mode 100644
index f4e144cb..00000000
--- a/Algorithms/LOOK/runLOOK.R
+++ /dev/null
@@ -1,30 +0,0 @@
-unloadNamespace("rlookc")
-library("rlookc")
-args <- commandArgs(trailingOnly = T)
-inFile <- args[1]
-outFile <-  args[2]
-
-# input expression data
-inputExpr <- read.table(inFile, sep=",", header = 1, row.names = 1)
-inputExpr = matrix(rnorm(1e3), nrow = 10, ncol = 100)
-rownames(inputExpr) = LETTERS[1:10]
-geneNames <- rownames(inputExpr)
-rownames(inputExpr) <- c(geneNames)
-
-knockoffResults = rlookc::generateLooks(t(inputExpr), mu = 0, Sigma = cor(t(inputExpr)), 
-                                        statistic = knockoff::stat.lasso_lambdasmax, 
-                                        output_type = "statistics")
-DF = list()
-for(i in seq(nrow(inputExpr))){
-  DF[[i]] = data.frame(
-    Gene1 = geneNames[ i],
-    Gene2 = geneNames[-i],
-    knockoff_stat = knockoffResults[[i]], 
-    q_value = rlookc::knockoff.qvals(knockoffResults[[i]])
-  )
-}
-DF = data.table::rbindlist(DF)
-# Write output to a file
-# https://stackoverflow.com/questions/38664241/ranking-and-counting-matrix-elements-in-r
-outDF <- DF[order(DF$q_value, decreasing=FALSE), ]
-write.table(outDF, outFile, sep = "\t", quote = FALSE, row.names = FALSE)
diff --git a/BLRun/lookRunner.py b/BLRun/lookRunner.py
deleted file mode 100644
index 4a1bc616..00000000
--- a/BLRun/lookRunner.py
+++ /dev/null
@@ -1,63 +0,0 @@
-import os
-import pandas as pd
-from pathlib import Path
-import numpy as np
-
-def generateInputs(RunnerObj):
-    '''
-    Function to generate desired inputs for LOOK.
-    If the folder/files under RunnerObj.datadir exist, 
-    this function will not do anything.
-    '''
-    if not RunnerObj.inputDir.joinpath("LOOK").exists():
-        print("Input folder for LOOK does not exist, creating input folder...")
-        RunnerObj.inputDir.joinpath("LOOK").mkdir(exist_ok = False)
-        
-    if not RunnerObj.inputDir.joinpath("LOOK/ExpressionData.csv").exists():
-        ExpressionData = pd.read_csv(RunnerObj.inputDir.joinpath(RunnerObj.exprData),
-                                     header = 0, index_col = 0)
-        
-        newExpressionData = ExpressionData.copy()
-        
-        # Write .csv file
-        newExpressionData.to_csv(RunnerObj.inputDir.joinpath("LOOK/ExpressionData.csv"),
-                             sep = ',', header  = True, index = True)
-    
-def run(RunnerObj):
-    '''
-    Function to run LOOK algorithm
-    '''
-    inputPath = "data" + str(RunnerObj.inputDir).split(str(Path.cwd()))[1] + \
-                    "/LOOK/ExpressionData.csv"
-    
-    # make output dirs if they do not exist:
-    outDir = "outputs/"+str(RunnerObj.inputDir).split("inputs/")[1]+"/LOOK/"
-    os.makedirs(outDir, exist_ok = True)
-    
-    outPath = "data/" +  str(outDir) + 'outFile.txt'
-    cmdToRun = ' '.join(['docker run --rm -v', str(Path.cwd())+':/data/ look:base /bin/sh -c \"time -v -o', "data/" + str(outDir) + 'time.txt', 'Rscript runLOOK.R',
-                         inputPath, outPath, '\"'])
-    print(cmdToRun)
-    os.system(cmdToRun)
-
-
-
-def parseOutput(RunnerObj):
-    '''
-    Function to parse outputs from LOOK.
-    '''
-    # Quit if output directory does not exist
-    outDir = "outputs/"+str(RunnerObj.inputDir).split("inputs/")[1]+"/LOOK/"
-    if not Path(outDir+'outFile.txt').exists():
-        print(outDir+'outFile.txt'+'does not exist, skipping...')
-        return
-        
-    # Read output
-    OutDF = pd.read_csv(outDir+'outFile.txt', sep = '\t', header = 0)
-    outFile = open(outDir + 'rankedEdges.csv','w')
-    outFile.write('Gene1'+'\t'+'Gene2'+'\t'+'EdgeWeight'+'\n')
-
-    for idx, row in OutDF.sort_values('q_value', ascending = True).iterrows():
-        outFile.write('\t'.join([row['Gene1'],row['Gene2'],str(-np.log10(row['q_value']))])+'\n')
-    outFile.close()
-    
diff --git a/BLRun/lookRunner.py~ b/BLRun/lookRunner.py~
deleted file mode 100644
index 4613a777..00000000
--- a/BLRun/lookRunner.py~
+++ /dev/null
@@ -1,63 +0,0 @@
-import os
-import pandas as pd
-from pathlib import Path
-import numpy as np
-
-def generateInputs(RunnerObj):
-    '''
-    Function to generate desired inputs for LOOK.
-    If the folder/files under RunnerObj.datadir exist, 
-    this function will not do anything.
-    '''
-    if not RunnerObj.inputDir.joinpath("LOOK").exists():
-        print("Input folder for LOOK does not exist, creating input folder...")
-        RunnerObj.inputDir.joinpath("LOOK").mkdir(exist_ok = False)
-        
-    if not RunnerObj.inputDir.joinpath("LOOK/ExpressionData.csv").exists():
-        ExpressionData = pd.read_csv(RunnerObj.inputDir.joinpath(RunnerObj.exprData),
-                                     header = 0, index_col = 0)
-        
-        newExpressionData = ExpressionData.copy()
-        
-        # Write .csv file
-        newExpressionData.to_csv(RunnerObj.inputDir.joinpath("LOOK/ExpressionData.csv"),
-                             sep = ',', header  = True, index = True)
-    
-def run(RunnerObj):
-    '''
-    Function to run LOOK algorithm
-    '''
-    inputPath = "data" + str(RunnerObj.inputDir).split(str(Path.cwd()))[1] + \
-                    "/LOOK/ExpressionData.csv"
-    
-    # make output dirs if they do not exist:
-    outDir = "outputs/"+str(RunnerObj.inputDir).split("inputs/")[1]+"/LOOK/"
-    os.makedirs(outDir, exist_ok = True)
-    
-    outPath = "data/" +  str(outDir) + 'outFile.txt'
-    cmdToRun = ' '.join(['docker run --rm -v', str(Path.cwd())+':/data/ look:base /bin/sh -c \"time -v -o', "data/" + str(outDir) + 'time.txt', 'Rscript runLOOK.R',
-                         inputPath, outPath, '\"'])
-    print(cmdToRun)
-    os.system(cmdToRun)
-
-
-
-def parseOutput(RunnerObj):
-    '''
-    Function to parse outputs from LOOK.
-    '''
-    # Quit if output directory does not exist
-    outDir = "outputs/"+str(RunnerObj.inputDir).split("inputs/")[1]+"/LOOK/"
-    if not Path(outDir+'outFile.txt').exists():
-        print(outDir+'outFile.txt'+'does not exist, skipping...')
-        return
-        
-    # Read output
-    OutDF = pd.read_csv(outDir+'outFile.txt', sep = '\t', header = 0)
-    outFile = open(outDir + 'rankedEdges.csv','w')
-    outFile.write('Gene1'+'\t'+'Gene2'+'\t'+'EdgeWeight'+'\n')
-
-    for idx, row in part1.sort_values('q', ascending = True).iterrows():
-        outFile.write('\t'.join([row['Gene1'],row['Gene2'],str(-log10(row['q']))])+'\n')
-    outFile.close()
-    
diff --git a/BLRun/runner.py b/BLRun/runner.py
index f3f76db7..d58c0eb5 100644
--- a/BLRun/runner.py
+++ b/BLRun/runner.py
@@ -8,7 +8,6 @@ import BLRun.grnboost2Runner as GRNBOOST2
 import BLRun.leapRunner as LEAP
 import BLRun.jump3Runner as JUMP3
 import BLRun.ppcorRunner as PPCOR
-import BLRun.lookRunner as LOOK
 import BLRun.grisliRunner as GRISLI
 import BLRun.singeRunner as SINGE
 import BLRun.scribeRunner as SCRIBE
@@ -26,7 +25,6 @@ InputMapper = {'SCODE':SCODE.generateInputs,
                'LEAP':LEAP.generateInputs,
                'JUMP3':JUMP3.generateInputs,
                'PPCOR':PPCOR.generateInputs,
-               'LOOK':LOOK.generateInputs,
                'GRISLI':GRISLI.generateInputs,
                'SINGE':SINGE.generateInputs,
                'SCRIBE':SCRIBE.generateInputs}
@@ -45,7 +43,6 @@ AlgorithmMapper = {'SCODE':SCODE.run,
             'LEAP':LEAP.run,
             'JUMP3':JUMP3.run,
             'PPCOR':PPCOR.run,
-            'LOOK':LOOK.run,
             'GRISLI':GRISLI.run,
             'SINGE':SINGE.run,
             'SCRIBE':SCRIBE.run}
@@ -62,8 +59,7 @@ OutputParser = {'SCODE':SCODE.parseOutput,
             'GRNBOOST2':GRNBOOST2.parseOutput,
             'LEAP': LEAP.parseOutput,
             'JUMP3': JUMP3.parseOutput,
-            'PPCOR':PPCOR.parseOutput,            
-            'LOOK':LOOK.parseOutput,
+            'PPCOR':PPCOR.parseOutput,
             'GRISLI':GRISLI.parseOutput,
             'SINGE':SINGE.parseOutput,
             'SCRIBE':SCRIBE.parseOutput}
diff --git a/BLRunner.py b/BLRunner.py
index 900edecc..e958d5be 100644
--- a/BLRunner.py
+++ b/BLRunner.py
@@ -24,7 +24,7 @@ import os
 import pandas as pd
 
 import BLRun as br
-
+yaml.warnings({'YAMLLoadWarning': False})
 
 
 def get_parser() -> argparse.ArgumentParser:
diff --git a/README.md b/README.md
index c68e1a87..b081921d 100644
--- a/README.md
+++ b/README.md
@@ -4,14 +4,12 @@
 This is the main repository for BEELINE. The documentation is available at: [https://murali-group.github.io/Beeline/](https://murali-group.github.io/Beeline/).
 
 Quick setup:
-
 - To install docker on Ubuntu 18.04, follow the steps mentioned [here](https://www.digitalocean.com/community/tutorials/how-to-install-and-use-docker-on-ubuntu-18-04)
 - Setup docker to run docker without sudo using ` sudo usermod -aG docker $USER`, if you haven't already. See more details [here](https://askubuntu.com/questions/477551/how-can-i-use-docker-without-sudo)
 - We recommend using [Anaconda](https://www.anaconda.com/) for Python. Run the `. setupAnacondaVENV.sh` command to automatically create an Anaconda virtual environment named BEELINE from requirements.txt and install necessary libraries required to run BEELINE. Alternatively, you can create virtual environment for python using vnev from requirements.txt as detailed [here](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/)
 
 
 We provided an example dataset under inputs/example/GSD/ and a corresponding configuration file necessary for running GRN inference using 12 methods described in BEELINE. 
-
 - To compute proposed reconstructions on the example dataset, run `python BLRunner.py --config config-files/config.yaml`. Running this script for the first time can be slow as it involves downloading the contianers from Docker hub.
 - To compute areas under the ROC and PR curves for the proposed reconstructions, run `python BLEvaluator.py --config config-files/config.yaml --auc`. To display the complete list of evalutation options, run `python BLEvaluator.py --help`.
 
diff --git a/config-files/config.yaml b/config-files/config.yaml
index 4199f9d6..12040d22 100644
--- a/config-files/config.yaml
+++ b/config-files/config.yaml
@@ -38,24 +38,24 @@ input_settings:
               
         - name: "PIDC"
           params: 
-              should_run: [False]
+              should_run: [True]
 
 
         - name: "GRNVBEM"
           params: 
-              should_run: [False]
+              should_run: [True]
 
               
 
         - name: "GENIE3"
           params: 
-              should_run: [False]
+              should_run: [True]
               
               
               
         - name: "GRNBOOST2"
           params: 
-              should_run: [False]
+              should_run: [True]
               
               
         - name: "PPCOR"
@@ -65,13 +65,10 @@ input_settings:
               # Used in parsing output
               pVal: [0.01]
               
-        - name: "LOOK"
-          params: 
-              should_run: [True]
                
         - name: "SCODE"
           params:
-              should_run: [False]
+              should_run: [True]
               z: [10]
               nIter: [1000]
               nRep: [6]
@@ -83,20 +80,20 @@ input_settings:
               
         - name: "SINCERITIES"
           params: 
-              should_run: [False]
+              should_run: [True]
               nBins: [10]
               
               
         - name: "LEAP"
           params: 
-              should_run: [False]
+              should_run: [True]
               # Default maxLag value is 0.33
               maxLag: [0.33]
              
               
         - name: "GRISLI"
           params: 
-              should_run: [False]
+              should_run: [True]
               L: [10]
               R: [3000]
               alphaMin: [0.0]
@@ -104,7 +101,7 @@ input_settings:
 
         - name: "SINGE"
           params: 
-              should_run: [False]
+              should_run: [True]
               lambda: [0.01]
               dT: [15]
               num_lags: [5]
@@ -117,7 +114,7 @@ input_settings:
 
         - name: "SCRIBE"
           params: 
-              should_run: [False]
+              should_run: [True]
               ### required parameters
               # a list of delay values
               delay: ["5"]
diff --git a/initialize.sh b/initialize.sh
index b637b61f..d63dd91e 100755
--- a/initialize.sh
+++ b/initialize.sh
@@ -6,63 +6,59 @@ echo "This may take a while..."
 BASEDIR=$(pwd)
 
 # You may remove the -q flag if you want to see the docker build status
-# cd $BASEDIR/Algorithms/ARBORETO
-# docker build -q -t arboreto:base .
-# echo "Docker container for ARBORETO is built and tagged as arboreto:base"
+cd $BASEDIR/Algorithms/ARBORETO
+docker build -q -t arboreto:base .
+echo "Docker container for ARBORETO is built and tagged as arboreto:base"
 
 
-# cd $BASEDIR/Algorithms/GRISLI/
-# docker build -q -t grisli:base .
-# echo "Docker container for GRISLI is built and tagged as grisli:base"
+cd $BASEDIR/Algorithms/GRISLI/
+docker build -q -t grisli:base .
+echo "Docker container for GRISLI is built and tagged as grisli:base"
 
 
-# cd $BASEDIR/Algorithms/GRNVBEM/
-# docker build -q -t grnvbem:base .
-# echo "Docker container for GRNVBEM is built and tagged as  grnvbem:base"
+cd $BASEDIR/Algorithms/GRNVBEM/
+docker build -q -t grnvbem:base .
+echo "Docker container for GRNVBEM is built and tagged as  grnvbem:base"
 
-# cd $BASEDIR/Algorithms/JUMP3/
-# docker build -q -t jump3:base .
-# echo "Docker container for JUMP3 is built and tagged as  jump3:base"
+cd $BASEDIR/Algorithms/JUMP3/
+docker build -q -t jump3:base .
+echo "Docker container for JUMP3 is built and tagged as  jump3:base"
 
-# cd $BASEDIR/Algorithms/LEAP/
-# docker build -q -t leap:base .
-# echo "Docker container for LEAP is built and tagged as  leap:base"
+cd $BASEDIR/Algorithms/LEAP/
+docker build -q -t leap:base .
+echo "Docker container for LEAP is built and tagged as  leap:base"
 
-# cd $BASEDIR/Algorithms/PIDC/
-# docker build -q -t pidc:base .
-# echo "Docker container for PIDC is built and tagged as pidc:base"
+cd $BASEDIR/Algorithms/PIDC/
+docker build -q -t pidc:base .
+echo "Docker container for PIDC is built and tagged as pidc:base"
 
-# cd $BASEDIR/Algorithms/PNI/
-# docker build -q -t pni:base .
-# echo "Docker container for PNI is built and tagged as pni:base"
+cd $BASEDIR/Algorithms/PNI/
+docker build -q -t pni:base .
+echo "Docker container for PNI is built and tagged as pni:base"
 
 cd $BASEDIR/Algorithms/PPCOR/
 docker build -q -t ppcor:base .
 echo "Docker container for PPCOR is built and tagged as ppcor:base"
 
-cd $BASEDIR/Algorithms/LOOK/
-docker build -q -t look:base .
-echo "Docker container for LOOK is built and tagged as look:base"
+cd $BASEDIR/Algorithms/SINGE/
+docker build -q -t singe:base .
+echo "Docker container for SINGE is built and tagged as singe:base"
 
-# cd $BASEDIR/Algorithms/SINGE/
-# docker build -q -t singe:base .
-# echo "Docker container for SINGE is built and tagged as singe:base"
+cd $BASEDIR/Algorithms/SCNS/
+docker build -q -t scns:base .
+echo "Docker container for SCNS is built and tagged as scns:base"
 
-# cd $BASEDIR/Algorithms/SCNS/
-# docker build -q -t scns:base .
-# echo "Docker container for SCNS is built and tagged as scns:base"
+cd $BASEDIR/Algorithms/SCODE/
+docker build -q -t scode:base .
+echo "Docker container for SCODE is built and tagged as scode:base"
 
-# cd $BASEDIR/Algorithms/SCODE/
-# docker build -q -t scode:base .
-# echo "Docker container for SCODE is built and tagged as scode:base"
+cd $BASEDIR/Algorithms/SCRIBE/
+docker build -q -t scribe:base .
+echo "Docker container for SCRIBE is built and tagged as sincerities:base"
 
-# cd $BASEDIR/Algorithms/SCRIBE/
-# docker build -q -t scribe:base .
-# echo "Docker container for SCRIBE is built and tagged as sincerities:base"
-
-# cd $BASEDIR/Algorithms/SINCERITIES/
-# docker build -q -t sincerities:base .
-# echo "Docker container for SINCERITIES is built and tagged as sincerities:base"
+cd $BASEDIR/Algorithms/SINCERITIES/
+docker build -q -t sincerities:base .
+echo "Docker container for SINCERITIES is built and tagged as sincerities:base"
 
 cd $BASEDIR
 
